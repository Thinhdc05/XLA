# ==============================================================================
# VISUALIZATION PIPELINE - Hi·ªÉn th·ªã t·ª´ng b∆∞·ªõc x·ª≠ l√Ω ·∫£nh
# Ch·ª©ng minh hi·ªÉu qu√° tr√¨nh x·ª≠ l√Ω ·∫£nh (Y√äU C·∫¶U ƒê·ªÄ B√ÄI)
# ==============================================================================

import cv2
import numpy as np
from scipy.ndimage import center_of_mass
import matplotlib.pyplot as plt
import streamlit as st

# ==============================================================================
# PIPELINE VISUALIZATION - 10 B∆Ø·ªöC CHI TI·∫æT
# ==============================================================================

def visualize_pipeline(image, mode='digit'):
    """
    Visualize to√†n b·ªô pipeline x·ª≠ l√Ω ·∫£nh v·ªõi 10 b∆∞·ªõc chi ti·∫øt
    
    Args:
        image: Input image (PIL or numpy array)
        mode: 'digit' or 'shape' - T·ªëi ∆∞u kh√°c nhau
    
    Returns:
        Dictionary ch·ª©a t·∫•t c·∫£ b∆∞·ªõc trung gian v√† th√¥ng tin
    """
    
    # Convert to numpy if needed
    if hasattr(image, 'mode'):  # PIL Image
        img = np.array(image)
    else:
        img = image.copy()
    
    pipeline = {}
    explanations = {}
    
    # ========== B∆Ø·ªöC 1: ORIGINAL ==========
    pipeline['step1_original'] = img.copy()
    explanations['step1'] = {
        'title': 'B∆∞·ªõc 1: ·∫¢nh g·ªëc (Original)',
        'description': '·∫¢nh ƒë·∫ßu v√†o t·ª´ camera/upload. C√≥ th·ªÉ m√†u ho·∫∑c grayscale, k√≠ch th∆∞·ªõc t√πy √Ω.'
    }
    
    # ========== B∆Ø·ªöC 2: GRAYSCALE ==========
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    else:
        gray = img.copy()
    
    # üÜï T·ª± ƒë·ªông ph√°t hi·ªán & ƒë·∫£o ng∆∞·ª£c n·∫øu n·ªÅn s√°ng (Paint/Upload)
    mean_brightness = gray.mean()
    is_light_background = mean_brightness > 127
    
    if is_light_background:
        gray = 255 - gray  # ƒê·∫£o ng∆∞·ª£c: n·ªÅn tr·∫Øng ‚Üí ƒëen, n√©t ƒëen ‚Üí tr·∫Øng
        invert_note = " ‚Üí Ph√°t hi·ªán n·ªÅn s√°ng (Paint/Upload), t·ª± ƒë·ªông ƒë·∫£o ng∆∞·ª£c."
    else:
        invert_note = " ‚Üí N·ªÅn t·ªëi (Canvas), gi·ªØ nguy√™n."
    
    pipeline['step2_grayscale'] = gray
    explanations['step2'] = {
        'title': 'B∆∞·ªõc 2: Grayscale + Auto Invert',
        'description': f'Chuy·ªÉn t·ª´ RGB sang grayscale. Mean brightness: {mean_brightness:.1f}.{invert_note} K√≠ch th∆∞·ªõc: {gray.shape}'
    }
    
    # ========== B∆Ø·ªöC 3: CLAHE (Contrast Enhancement) ==========
    clahe = cv2.createCLAHE(clipLimit=2.0 if mode == 'digit' else 3.0, 
                            tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)
    
    pipeline['step3_clahe'] = enhanced
    explanations['step3'] = {
        'title': 'B∆∞·ªõc 3: CLAHE - TƒÉng Contrast',
        'description': 'Contrast Limited Adaptive Histogram Equalization. C·∫£i thi·ªán contrast c·ª•c b·ªô, x·ª≠ l√Ω √°nh s√°ng kh√¥ng ƒë·ªÅu. ƒê·∫∑c bi·ªát t·ªët cho ·∫£nh t·ªëi ho·∫∑c m·ªù.'
    }
    
    # ========== B∆Ø·ªöC 4: DENOISING ==========
    if mode == 'shape':
        denoised = cv2.bilateralFilter(enhanced, 9, 75, 75)
        denoise_method = 'Bilateral Filter (gi·ªØ edge)'
    else:
        denoised = cv2.GaussianBlur(enhanced, (5, 5), 0)
        denoise_method = 'Gaussian Blur'
    
    pipeline['step4_denoised'] = denoised
    explanations['step4'] = {
        'title': 'B∆∞·ªõc 4: Kh·ª≠ nhi·ªÖu (Denoising)',
        'description': f'{denoise_method}. Lo·∫°i b·ªè nhi·ªÖu (noise) tr∆∞·ªõc khi threshold. L√†m m·ªãn ·∫£nh, gi·ªØ l·∫°i c·∫•u tr√∫c ch√≠nh.'
    }
    
    # ========== B∆Ø·ªöC 5: THRESHOLD ==========
    blocksize = 11 if mode == 'digit' else 13
    c_value = 2 if mode == 'digit' else 3
    
    thresh = cv2.adaptiveThreshold(
        denoised, 255, 
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY_INV,
        blockSize=blocksize, 
        C=c_value
    )
    
    pipeline['step5_threshold'] = thresh
    explanations['step5'] = {
        'title': 'B∆∞·ªõc 5: Adaptive Threshold',
        'description': f'Chuy·ªÉn sang ·∫£nh nh·ªã ph√¢n (binary). Adaptive threshold t·ª± ƒë·ªông t√≠nh ng∆∞·ª°ng cho t·ª´ng v√πng nh·ªè. T·ªët h∆°n threshold c·ªë ƒë·ªãnh. BlockSize={blocksize}, C={c_value}'
    }
    
    # ========== B∆Ø·ªöC 6: MORPHOLOGY ==========
    kernel_size = 3 if mode == 'digit' else 5
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
    
    # Adaptive Morphology d·ª±a tr√™n ƒë·ªô d√†y n√©t
    white_ratio = np.sum(thresh > 0) / thresh.size
    
    if white_ratio < 0.20:  # N√©t m·ªèng - TƒÇNG ng∆∞·ª°ng t·ª´ 15% ‚Üí 20%
        closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)
        cleaned = closed  # B·ªè OPEN ƒë·ªÉ gi·ªØ n√©t m·ªèng
        morph_desc = f'ADAPTIVE: N√©t m·ªèng ({white_ratio*100:.1f}% di·ªán t√≠ch) ‚Üí Ch·ªâ Closing, b·ªè Opening. Gi·ªØ nguy√™n n√©t m·∫£nh.'
    else:  # N√©t d√†y/b√¨nh th∆∞·ªùng
        closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)
        cleaned = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel, iterations=1)
        morph_desc = f'STANDARD: N√©t b√¨nh th∆∞·ªùng ({white_ratio*100:.1f}% di·ªán t√≠ch) ‚Üí Closing + Opening. Lo·∫°i nhi·ªÖu t·ªët.'
    
    pipeline['step6_morphology'] = cleaned
    explanations['step6'] = {
        'title': 'B∆∞·ªõc 6: Morphology Operations (Adaptive)',
        'description': morph_desc
    }
    
    # ========== B∆Ø·ªöC 7: CONTOUR DETECTION ==========
    contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # Draw contours for visualization
    contour_img = cv2.cvtColor(cleaned, cv2.COLOR_GRAY2RGB)
    h, w = cleaned.shape
    
    # L·ªçc contours h·ª£p l·ªá
    min_area = (h * w) * 0.01
    valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_area]
    
    merge_applied = False
    if len(valid_contours) > 0:
        cv2.drawContours(contour_img, valid_contours, -1, (0, 255, 0), 2)
        
        # N·∫øu c√≥ nhi·ªÅu contours, ki·ªÉm tra kho·∫£ng c√°ch tr∆∞·ªõc khi merge
        if len(valid_contours) > 1:
            bboxes = [cv2.boundingRect(cnt) for cnt in valid_contours]
            max_distance = max(h, w) * 0.4
            should_merge = False
            
            for i in range(len(bboxes)):
                for j in range(i+1, len(bboxes)):
                    x1, y1, w1, h1 = bboxes[i]
                    x2, y2, w2, h2 = bboxes[j]
                    cx1, cy1 = x1 + w1//2, y1 + h1//2
                    cx2, cy2 = x2 + w2//2, y2 + h2//2
                    distance = np.sqrt((cx1-cx2)**2 + (cy1-cy2)**2)
                    
                    if distance < max_distance:
                        should_merge = True
                        break
                if should_merge:
                    break
            
            if should_merge:
                all_points = np.vstack(valid_contours)
                x, y, cw, ch = cv2.boundingRect(all_points)
                cv2.rectangle(contour_img, (x, y), (x+cw, y+ch), (255, 0, 0), 3)
                merge_applied = True
            else:
                largest = max(valid_contours, key=cv2.contourArea)
                cv2.drawContours(contour_img, [largest], 0, (255, 0, 0), 3)
        else:
            cv2.drawContours(contour_img, [valid_contours[0]], 0, (255, 0, 0), 3)
    
    pipeline['step7_contours'] = contour_img
    explanations['step7'] = {
        'title': 'B∆∞·ªõc 7: Ph√°t hi·ªán Contour',
        'description': f'T√¨m bi√™n (contour) c·ªßa ƒë·ªëi t∆∞·ª£ng. T√¨m ƒë∆∞·ª£c {len(contours)} contours, {len(valid_contours)} h·ª£p l·ªá. {"Merge contours g·∫ßn nhau (s·ªë 8/0/6/9)." if merge_applied else "Ch·ªçn contour l·ªõn nh·∫•t."}'
    }
    
    # ========== B∆Ø·ªöC 8: CROP & EXTRACT ==========
    if len(valid_contours) > 0:
        # Ki·ªÉm tra kho·∫£ng c√°ch tr∆∞·ªõc khi merge
        if len(valid_contours) > 1:
            bboxes = [cv2.boundingRect(cnt) for cnt in valid_contours]
            max_distance = max(h, w) * 0.4
            should_merge = False
            
            for i in range(len(bboxes)):
                for j in range(i+1, len(bboxes)):
                    x1, y1, w1, h1 = bboxes[i]
                    x2, y2, w2, h2 = bboxes[j]
                    cx1, cy1 = x1 + w1//2, y1 + h1//2
                    cx2, cy2 = x2 + w2//2, y2 + h2//2
                    distance = np.sqrt((cx1-cx2)**2 + (cy1-cy2)**2)
                    
                    if distance < max_distance:
                        should_merge = True
                        break
                if should_merge:
                    break
            
            if should_merge:
                all_points = np.vstack(valid_contours)
                x, y, cw, ch = cv2.boundingRect(all_points)
            else:
                largest = max(valid_contours, key=cv2.contourArea)
                x, y, cw, ch = cv2.boundingRect(largest)
        else:
            x, y, cw, ch = cv2.boundingRect(valid_contours[0])
        
        padding = 15 if mode == 'digit' else 20
        x1 = max(0, x - padding)
        y1 = max(0, y - padding)
        x2 = min(w, x + cw + padding)
        y2 = min(h, y + ch + padding)
        
        cropped = cleaned[y1:y2, x1:x2]
    else:
        cropped = cleaned
    
    pipeline['step8_cropped'] = cropped
    explanations['step8'] = {
        'title': 'B∆∞·ªõc 8: Crop & Extract',
        'description': f'C·∫Øt (crop) ƒë·ªëi t∆∞·ª£ng theo bounding box + padding. Lo·∫°i b·ªè background kh√¥ng c·∫ßn thi·∫øt. K√≠ch th∆∞·ªõc crop: {cropped.shape}'
    }
    
    # ========== B∆Ø·ªöC 9: RESIZE TO 20x20 ==========
    if cropped.size > 0 and cropped.shape[0] > 0 and cropped.shape[1] > 0:
        resized = cv2.resize(cropped, (20, 20), interpolation=cv2.INTER_AREA)
    else:
        resized = np.zeros((20, 20), dtype=np.uint8)
    
    pipeline['step9_resized'] = resized
    explanations['step9'] = {
        'title': 'B∆∞·ªõc 9: Resize v·ªÅ 20x20',
        'description': 'Resize v·ªÅ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh 20x20 pixels. S·ª≠ d·ª•ng INTER_AREA (t·ªët cho downsampling). Chu·∫©n b·ªã cho padding.'
    }
    
    # ========== B∆Ø·ªöC 10: PAD TO 28x28 & CENTER ==========
    padded = np.pad(resized, ((4, 4), (4, 4)), 'constant', constant_values=0)
    
    # Center by center of mass
    if np.sum(padded) > 0:
        cy, cx = center_of_mass(padded)
        shiftx = int(np.round(14 - cx))
        shifty = int(np.round(14 - cy))
        M = np.float32([[1, 0, shiftx], [0, 1, shifty]])
        centered = cv2.warpAffine(padded, M, (28, 28))
    else:
        centered = padded
    
    pipeline['step10_final'] = centered
    explanations['step10'] = {
        'title': 'B∆∞·ªõc 10: Pad & Center v·ªÅ 28x28',
        'description': f'Padding th√™m 4 pixels m·ªói b√™n: 20x20‚Üí28x28. Center b·∫±ng center of mass. ƒê√¢y l√† input chu·∫©n cho CNN (28x28x1). S·∫µn s√†ng cho prediction!'
    }
    
    # ========== NORMALIZED FOR MODEL ==========
    normalized = centered.astype(np.float32) / 255.0
    pipeline['normalized'] = normalized.reshape(1, 28, 28, 1)
    
    return {
        'pipeline': pipeline,
        'explanations': explanations,
        'final_input': pipeline['normalized']
    }

# ==============================================================================
# HISTOGRAM COMPARISON
# ==============================================================================

def show_histogram_comparison(original, processed):
    """
    So s√°nh histogram tr∆∞·ªõc v√† sau x·ª≠ l√Ω
    
    Args:
        original: Original grayscale image
        processed: Processed grayscale image
    
    Returns:
        Matplotlib figure
    """
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # Original image
    axes[0, 0].imshow(original, cmap='gray')
    axes[0, 0].set_title('Original Image')
    axes[0, 0].axis('off')
    
    # Original histogram
    axes[0, 1].hist(original.ravel(), bins=256, range=[0, 256], color='blue', alpha=0.7)
    axes[0, 1].set_title('Original Histogram')
    axes[0, 1].set_xlabel('Pixel Intensity')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].grid(alpha=0.3)
    
    # Processed image
    axes[1, 0].imshow(processed, cmap='gray')
    axes[1, 0].set_title('Processed Image')
    axes[1, 0].axis('off')
    
    # Processed histogram
    axes[1, 1].hist(processed.ravel(), bins=256, range=[0, 256], color='green', alpha=0.7)
    axes[1, 1].set_title('Processed Histogram')
    axes[1, 1].set_xlabel('Pixel Intensity')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].grid(alpha=0.3)
    
    plt.tight_layout()
    return fig

# ==============================================================================
# EDGE DETECTION VISUALIZATION
# ==============================================================================

def visualize_edge_detection(image):
    """
    Visualize nhi·ªÅu ph∆∞∆°ng ph√°p edge detection
    
    Args:
        image: Grayscale image
    
    Returns:
        Dictionary with edge images
    """
    # Sobel
    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
    sobel = np.sqrt(sobel_x**2 + sobel_y**2)
    sobel = cv2.normalize(sobel, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
    
    # Canny
    canny = cv2.Canny(image, 50, 150)
    
    # Laplacian
    laplacian = cv2.Laplacian(image, cv2.CV_64F)
    laplacian = cv2.normalize(laplacian, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
    
    return {
        'sobel': sobel,
        'canny': canny,
        'laplacian': laplacian,
        'original': image
    }

# ==============================================================================
# STREAMLIT DISPLAY HELPERS
# ==============================================================================

def display_pipeline_streamlit(result):
    """
    Hi·ªÉn th·ªã pipeline trong Streamlit v·ªõi layout ƒë·∫πp
    
    Args:
        result: Output t·ª´ visualize_pipeline()
    """
    pipeline = result['pipeline']
    explanations = result['explanations']
    
    st.markdown("### üî¨ PIPELINE X·ª¨ L√ù ·∫¢NH - 10 B∆Ø·ªöC CHI TI·∫æT")
    
    # Display in grid: 2 columns x 5 rows
    steps = [
        ('step1_original', 'step1'),
        ('step2_grayscale', 'step2'),
        ('step3_clahe', 'step3'),
        ('step4_denoised', 'step4'),
        ('step5_threshold', 'step5'),
        ('step6_morphology', 'step6'),
        ('step7_contours', 'step7'),
        ('step8_cropped', 'step8'),
        ('step9_resized', 'step9'),
        ('step10_final', 'step10'),
    ]
    
    for i in range(0, len(steps), 2):
        col1, col2 = st.columns(2)
        
        # Left column
        if i < len(steps):
            img_key, exp_key = steps[i]
            with col1:
                # Hi·ªÉn th·ªã ·∫£nh original v·ªõi m√†u n·∫øu c√≥
                if img_key == 'step1_original' and len(pipeline[img_key].shape) == 3:
                    st.image(pipeline[img_key], width=250, channels='RGB')
                else:
                    st.image(pipeline[img_key], width=250, channels='GRAY' if len(pipeline[img_key].shape) == 2 else 'RGB')
                st.markdown(f"**{explanations[exp_key]['title']}**")
                st.caption(explanations[exp_key]['description'])
        
        # Right column
        if i + 1 < len(steps):
            img_key, exp_key = steps[i + 1]
            with col2:
                # Hi·ªÉn th·ªã ·∫£nh original v·ªõi m√†u n·∫øu c√≥
                if img_key == 'step1_original' and len(pipeline[img_key].shape) == 3:
                    st.image(pipeline[img_key], width=250, channels='RGB')
                else:
                    st.image(pipeline[img_key], width=250, channels='GRAY' if len(pipeline[img_key].shape) == 2 else 'RGB')
                st.markdown(f"**{explanations[exp_key]['title']}**")
                st.caption(explanations[exp_key]['description'])

# ==============================================================================
# EXPORT
# ==============================================================================

__all__ = [
    'visualize_pipeline',
    'show_histogram_comparison',
    'visualize_edge_detection',
    'display_pipeline_streamlit'
]
